.\"t
.\" Automatically generated by Pandoc 2.2.1
.\"
.TH "FF\-FORMAT" "5" "Sep 15, 2018" "" ""
.hy
.SH NAME
.PP
\f[B]ff\-format\f[] \[en] finalfrontier binary format
.SH DESCRIPTION
.PP
The finalfrontier binary format stores stores word embeddings.
The format contains the following parts:
.IP "1." 3
Header
.IP "2." 3
Configuration
.IP "3." 3
Vocabulary
.IP "4." 3
Embedding matrix
.IP "5." 3
L2 norms
.PP
All numerical values are stored in \f[I]little endian\f[] format.
The remainder of this page describes each part of file version
\f[I]3\f[] in more detail.
.SH HEADER
.PP
.TS
tab(@);
l l l.
T{
\f[B]Field\f[]
T}@T{
\f[B]Size\f[]
T}@T{
Notes
T}
_
T{
Magic number
T}@T{
2
T}@T{
The magic number is `FF'
T}
T{
Version
T}@T{
4
T}@T{
File format version (3)
T}
.TE
.SH CONFIGURATION
.PP
.TS
tab(@);
l l l.
T{
\f[B]Field\f[]
T}@T{
\f[B]Type\f[]
T}@T{
\f[B]Notes\f[]
T}
_
T{
Model type
T}@T{
u8
T}@T{
0: skipgram, 1: structgram
T}
T{
Loss type
T}@T{
u8
T}@T{
0: Logistic NS
T}
T{
Context size
T}@T{
u32
T}@T{
T}
T{
Dimensionality
T}@T{
u32
T}@T{
T}
T{
Discard threshold
T}@T{
f32
T}@T{
T}
T{
Epochs
T}@T{
u32
T}@T{
T}
T{
Min count
T}@T{
u32
T}@T{
T}
T{
Min N
T}@T{
u32
T}@T{
Minimal subword length
T}
T{
Max N
T}@T{
u32
T}@T{
Maximal subword length
T}
T{
Buckets EXP
T}@T{
u32
T}@T{
2^EXP buckets for subwords
T}
T{
Negative samples
T}@T{
u32
T}@T{
T}
T{
Initial learning rate
T}@T{
f32
T}@T{
T}
.TE
.SH VOCABULARY
.PP
.TS
tab(@);
l l l.
T{
\f[B]Field\f[]
T}@T{
\f[B]Type\f[]
T}@T{
\f[B]Notes\f[]
T}
_
T{
No.\ of corpus tokens
T}@T{
u64
T}@T{
T}
T{
Vocabulary size
T}@T{
u64
T}@T{
T}
T{
Word length
T}@T{
u32
T}@T{
See below
T}
T{
Word
T}@T{
UTF\-8
T}@T{
See below
T}
T{
Word count
T}@T{
u64
T}@T{
See below
T}
.TE
.IP \[bu] 2
The last three fields are repeated for every word in the vocabulary.
.IP \[bu] 2
The vocabulary is ordered by token frequency.
The ordering of tokens that are equally frequent is undefined.
.SH EMBEDDING MATRIX
.PP
.TS
tab(@);
l l.
T{
\f[B]Field\f[]
T}@T{
\f[B]Type\f[]
T}
_
T{
Word embeddings
T}@T{
vocab size * dimensionality * f32
T}
T{
Subword embeddings
T}@T{
2^EXP * dimensionality * f32
T}
.TE
.IP \[bu] 2
The word embeddings are normalized to unit vectors.
.IP \[bu] 2
The word embeddings are in vocabulary order.
.IP \[bu] 2
The subword embeddings are unnormalized.
.SH L2 NORMS
.PP
.TS
tab(@);
l l.
T{
\f[B]Field\f[]
T}@T{
\f[B]Type\f[]
T}
_
T{
l2 norms
T}@T{
vocab size * f32
T}
.TE
.PP
The original unnormalized word embeddings can be obtained by multiplying
each embedding by the corresponding l2 norm.
.SH SEE ALSO
.PP
ff\-convert(1), ff\-similar(1), ff\-train(1)
.SH AUTHORS
Daniel de Kok.
