.\" Automatically generated by Pandoc 2.2.1
.\"
.TH "FF\-TRAIN" "1" "Sep 8, 2018" "" ""
.hy
.SH NAME
.PP
\f[B]ff\-train\f[] \[en] train word embeddings with subword
representations
.SH SYNOPSIS
.PP
\f[B]ff\-train\f[] [\f[I]options\f[]] \f[I]corpus\f[]
\f[I]output_model\f[]
.SH DESCRIPTION
.PP
The \f[B]ff\-train\f[] trains word embeddings using data from a
\f[I]corpus\f[].
The corpus should have tokens separated by spaces and sentences
separated by newlines.
After training, the embeddings are written to \f[I]output_model\f[].
.SH OPTIONS
.TP
.B \f[C]\-\-buckets\f[] \f[I]EXP\f[]
The bucket exponent.
finalfrontier uses 2^\f[I]EXP\f[] buckets to store subword
representations.
Each subword representation (n\-gram) is hashed and mapped to a bucket
based on this hash.
Using more buckets will result in fewer bucket collisions between
subword representations at the cost of memory use.
The default bucket exponent is \f[I]21\f[] (approximately 2 million
buckets).
.RS
.RE
.TP
.B \f[C]\-\-context\f[] \f[I]CONTEXT_SIZE\f[]
Words within the \f[I]CONTEXT_SIZE\f[] of a focus word will be used to
learn the representation of the focus word.
The default context size is \f[I]5\f[].
.RS
.RE
.TP
.B \f[C]\-\-discard\f[] \f[I]THRESHOLD\f[]
The discard threshold influences how often frequent words are discarded
from training.
The default discard threshold is \f[I]1e\-4\f[].
.RS
.RE
.TP
.B \f[C]\-\-epochs\f[] \f[I]N\f[]
The number of training epochs.
The number of necessary training epochs typically decreases with the
corpus size.
The default number of epochs is \f[I]5\f[].
.RS
.RE
.TP
.B \f[C]\-\-lr\f[] \f[I]LEARNING_RATE\f[]
The learning rate determines what fraction of a gradient is used for
parameter updates.
The default initial learning rate is \f[I]0.05\f[], the learning rate
decreases monotonically during training.
.RS
.RE
.TP
.B \f[C]\-\-maxn\f[] \f[I]LEN\f[]
The maximum n\-gram length for subword representations.
Default: 6
.RS
.RE
.TP
.B \f[C]\-\-mincount\f[] \f[I]FREQ\f[]
The minimum count controls discarding of infrequent.
Words occuring fewer than \f[I]FREQ\f[] times are not considered during
training.
The default minimum count is 5.
.RS
.RE
.TP
.B \f[C]\-\-minn\f[] \f[I]LEN\f[]
The minimum n\-gram length for subword representations.
Default: 3
.RS
.RE
.TP
.B \f[C]\-\-model\f[] \f[I]MODEL\f[]
The model to use for training word embeddings.
The choices here are \f[I]skipgram\f[] for the skip\-gram model (Mikolov
et al., 2013) and \f[I]structgram\f[] for the stuctured skip\-gram model
(Ling et al.\ 2015).
.RS
.PP
The structured skip\-gram model takes the position of a context word
into account and results in embeddings that are typically better suited
for syntax\-oriented tasks.
.PP
The default model is \f[I]skipgram\f[].
.RE
.TP
.B \f[C]\-\-ns\f[] \f[I]FREQ\f[]
The number of negatives to sample per positive example.
Default: 5
.RS
.RE
.TP
.B \f[C]\-\-threads\f[] \f[I]N\f[]
The number of thread to use during training for parallelization.
The default is to use half of the logical CPUs of the machine.
.RS
.RE
.SH EXAMPLES
.PP
Train embeddings on \f[I]dewiki.txt\f[] using the skip\-gram model:
.IP
.nf
\f[C]
ff\-train\ dewiki.txt\ dewiki\-skipgram.bin
\f[]
.fi
.PP
Train embeddings with dimensionality 100 on \f[I]dewiki.txt\f[] using
the structured skip\-gram model:
.IP
.nf
\f[C]
ff\-train\ \-\-model\ structgram\ \-\-dims\ 100\ \\
\ \ dewiki.txt\ dewiki\-structgram.bin
\f[]
.fi
.SH SEE ALSO
.PP
ff\-convert(1), ff\-similar(1)
.SH AUTHORS
Daniel de Kok.
